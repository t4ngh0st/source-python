import requests
from bs4 import BeautifulSoup
import praw
from googlesearch import search
import config.chpy as config
from urllib.parse import quote_plus
from time import sleep

class SearchTools:
    def __init__(self):
        # Setup Reddit API (gunakan credential dari config)
        self.reddit = praw.Reddit(
            client_id=config.REDDIT_API["CLIENT_ID"],
            client_secret=config.REDDIT_API["CLIENT_SECRET"],
            user_agent=config.REDDIT_API["USER_AGENT"]
        )

    def google_dork(self, query, num_results=5):
        """Melakukan Google dorking"""
        dork_results = []
        try:
            for url in search(f'site:{query}', 
                            num_results=num_results,
                            advanced=True):
                dork_results.append({
                    'title': url.title,
                    'url': url.url,
                    'description': url.description
                })
                sleep(1)  # Delay untuk menghindari blocking
            return dork_results
        except Exception as e:
            return f"Error: {str(e)}"

    def search_reddit(self, query, limit=5):
        """Mencari post pentest di Reddit"""
        results = []
        try:
            for submission in self.reddit.subreddit("netsec+AskNetsec").search(
                query, limit=limit, sort='relevance'):
                results.append({
                    'title': submission.title,
                    'url': submission.url,
                    'score': submission.score,
                    'comments': submission.num_comments
                })
            return results
        except Exception as e:
            return f"Error: {str(e)}"

    def search_exploit_db(self, query):
        """Mencari di Exploit Database"""
        try:
            url = f"https://www.exploit-db.com/search?q={quote_plus(query)}"
            headers = {'User-Agent': 'Mozilla/5.0'}
            response = requests.get(url, headers=headers)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            exploits = []
            for row in soup.select('table.table-striped tbody tr'):
                cols = row.find_all('td')
                exploits.append({
                    'title': cols[1].get_text(strip=True),
                    'url': f"https://www.exploit-db.com{cols[1].a['href']}",
                    'date': cols[2].get_text(strip=True),
                    'type': cols[3].get_text(strip=True)
                })
            return exploits[:5]  # Batasi 5 hasil
        except Exception as e:
            return f"Error: {str(e)}"